{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise n.2\n",
    "Assume you are at the end of a project and delivered the prediction results of a regression task\n",
    "by applying a model (e.g. DNN, RF, GAM, etc.). Your business partner asks now after a numerical uncertainty quantification of your prediction results.\n",
    "Please come up with two or three approaches to do so, and choose one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Answers:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #1\n",
    "\n",
    "We can train multiple different models and treat the predicitons all together as a distribution. Then we can compute standard deviation and percentiles of interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #2:\n",
    "In statistic language, the linear regression model can be seen as a $y = XW + e$, where $e$ ~ $N(0, σ^2)$ or $y$ ~ $N(XW, σ^2)$.\n",
    "\n",
    "When we use Maximum Likelihood Estimation to make the observed data most likely to appear, we are in the end solving the least squre error minimization problem. \n",
    "\n",
    "One idea would be parameterize the $σ$ and use Maximum Likelihood Estimation or $y$ ~ $N(XW, (XQ)^2)$.\n",
    "\n",
    "We can also use neural network to model the mean and standard deviation (replace $XW$ with $f(x)$ and $XQ$ with $g(x)$, where $f(.)$ and $g(. )$ are neural neworks). \n",
    "\n",
    "We might model the $log(σ^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach #3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During **ICML'18@Stockholm**, I have encountered a paper with title \"[Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning](https://arxiv.org/pdf/1710.07283.pdf)\".\n",
    "\n",
    "In this paper, the authors investigated the uncertainty and risk with bayesian deep learning (paramters are stochastic variables).\n",
    "\n",
    "The paper is quite theritical and a new subject to me. So I didn't really follow all the details. But the main take away is that, with neural network, it is possible to decompose uncertainty into (1). Aleatoric uncertainty(statistical uncertainty) and (2). Epistemic uncertainty (systematic uncertainty). It could be useful for risk sensitive applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
